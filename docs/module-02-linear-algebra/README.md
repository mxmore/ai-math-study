# Module 02：线性代数与向量空间

## 模块简介
掌握向量、矩阵及线性变换的核心概念，为理解线性模型、特征分解与优化打基础。

## 学习目标
- 能解释向量、矩阵运算及矩阵乘法的几何意义。
- 能判断线性相关性与秩，理解基与维度的概念。
- 能理解正交化与正交矩阵的性质，手动实现 Gram-Schmidt。
- 能计算特征值/特征向量，理解对角化与 SVD 的直觉。
- 能使用线性代数解决最小二乘问题与线性回归闭式解。

## 核心概念
- 向量空间与基：描述空间的生成方式与维度。
- 矩阵乘法与线性变换：旋转、缩放、投影的几何直观。
- 秩与线性相关：列/行空间的维度，决定方程可解性。
- 正交化与正交矩阵：长度保持、数值稳定性。
- 特征分解与 SVD：分解矩阵结构，连接 PCA、压缩与降维。

## 推荐学习顺序
1. 向量与矩阵基本运算，矩阵乘法规则与几何意义。
2. 线性相关、秩、基与维度。
3. 正交化（Gram-Schmidt）与正交矩阵性质。
4. 特征值/特征向量、对称矩阵的特征分解。
5. 奇异值分解（SVD）与应用。
6. 最小二乘与线性回归闭式解。

## 配套编程练习
- 使用 numpy 实现向量点积、矩阵乘法，并通过绘图展示二维旋转/缩放效果。
- 手动实现 Gram-Schmidt，验证结果是否正交。
- 对称矩阵求特征分解，解释特征向量方向与伸缩关系。
- 使用 SVD 对图像进行降维压缩，对比不同秩的重建误差。
- 使用正规方程实现线性回归，比较与梯度下降的差异。

## 与机器学习的联系
- 线性回归、PCA、推荐系统、词向量等都依赖线性代数。
- 特征分解与 SVD 是降维与压缩的核心工具。
- 正交化与数值稳定性直接影响优化与训练精度。

## 自我检测问题
1. 为什么矩阵乘法代表线性变换的组合？
2. 如何判断一组向量是否线性相关？
3. Gram-Schmidt 的步骤是什么？为什么要进行归一化？
4. 对称矩阵为什么总能被正交对角化？
5. SVD 中的奇异值和特征值有什么关系？
6. 正规方程解线性回归时，`(X^T X)` 可逆的条件是什么？
7. 如果 `(X^T X)` 不可逆，有哪些替代方案？
8. 如何用几何方式解释最小二乘解？
