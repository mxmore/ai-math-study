# Module 05：优化基础与梯度方法

## 模块简介
介绍无约束优化的一阶方法，理解学习率、正则化对收敛与泛化的影响，并通过线性/逻辑回归实例体验实现细节。

## 学习目标
- 能推导并实现一元与多元函数的梯度下降。
- 能理解学习率、迭代次数与收敛速度的关系。
- 能解释动量、Adam 等一阶自适应方法的直觉。
- 能理解 L1/L2 正则化的作用及偏差-方差权衡。
- 能用可视化方式呈现损失曲线与参数更新轨迹。

## 核心概念
- 梯度下降：沿负梯度方向更新以减少目标函数。
- 学习率与收敛：步长过大/过小的影响，线搜索直觉。
- 动量与自适应方法：平滑梯度、缩放学习率以加速收敛。
- 正则化：L1 稀疏化、L2 平滑化，控制模型复杂度。

## 推荐学习顺序
1. 梯度下降推导与一维示例。
2. 多元函数梯度下降与批量/随机梯度（BGD/SGD）。
3. 动量、Adam 等改进方法的公式与直觉。
4. 正则化与偏差-方差权衡。
5. 在线性/逻辑回归上实践训练与可视化收敛曲线。

## 配套编程练习
- 从零实现线性回归的梯度下降，绘制损失随迭代的变化。
- 对比不同学习率下的收敛与震荡情况。
- 在噪声数据上添加 L2 正则，观察系数衰减与过拟合变化。
- 实现简单的逻辑回归训练，绘制决策边界与损失曲线。

## 与机器学习的联系
- 深度学习的训练依赖梯度下降及其变体。
- 正则化与优化超参数直接影响模型泛化能力。
- 收敛性与数值稳定性决定训练效率与可复现性。

## 自我检测问题
1. 梯度下降的更新公式是什么？如何选择学习率？
2. BGD、SGD、Mini-batch SGD 的区别与优劣？
3. 动量方法如何缓解震荡并加速收敛？
4. Adam 如何结合动量与自适应学习率？
5. 为什么 L2 正则化会使权重趋向较小的值？
6. 如何判断优化过程是否收敛或需要调整超参数？
7. 对逻辑回归，交叉熵损失的梯度形式是什么？
8. 在噪声数据上，正则化如何改善泛化？
