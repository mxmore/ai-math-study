# Module 06：信息论与机器学习连接

## 模块简介
介绍熵、交叉熵与 KL 散度等信息论概念，理解它们在分类、语言模型等任务中的角色，建立概率模型与损失函数之间的联系。

## 学习目标
- 能解释熵、联合熵、条件熵的含义与计算方式。
- 能理解交叉熵与 KL 散度的定义及其非负性。
- 能将交叉熵损失与分类问题的概率解释联系起来。
- 能推导逻辑回归与 softmax 中的交叉熵目标函数。
- 能通过简单实验对比 MSE 与交叉熵在分类中的表现。

## 核心概念
- 熵：度量不确定性，信息量的期望。
- 交叉熵：衡量真实分布与预测分布的差异，等价于 NLL。
- KL 散度：非对称的分布差异度量，连接最大似然与最小化 KL。
- Softmax 与对数几率：将线性输出映射到概率分布。

## 推荐学习顺序
1. 熵与信息量，离散分布示例。
2. 交叉熵的定义与与似然的关系。
3. KL 散度及其性质，与交叉熵的关系。
4. 逻辑回归与二分类交叉熵；softmax 多分类交叉熵推导。
5. 实验：对比 MSE 与交叉熵在分类任务上的优化表现。

## 配套编程练习
- 计算简单离散分布的熵，观察熵随分布均匀程度的变化。
- 实现二分类交叉熵与 softmax 多分类交叉熵函数。
- 用模拟数据训练 softmax 回归，绘制决策边界与损失下降曲线。
- 在同一分类任务上对比 MSE 与交叉熵的训练效果与收敛速度。

## 与机器学习的联系
- 交叉熵是分类任务的主流损失函数；最小化交叉熵等价于最大似然。
- KL 散度是变分推断、信息瓶颈、GAN 等方法的核心度量。
- 熵与信息量的概念帮助理解正则化、剪枝、模型选择。

## 自我检测问题
1. 熵的定义是什么？为什么熵越大表示不确定性越高？
2. 交叉熵与最大似然之间的联系是什么？
3. KL 散度为什么不对称？
4. 对二分类逻辑回归，交叉熵损失的公式是什么？
5. Softmax 的输出如何与交叉熵结合用于多分类？
6. 为什么 MSE 不适合处理概率输出的分类任务？
7. KL 散度与交叉熵的关系公式是什么？
8. 交叉熵损失与信息论中的编码长度有什么关系？
