{
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# Module 06：信息论与交叉熵练习\n", "通过计算熵与训练简单分类模型理解信息量。"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 1. 计算离散分布的熵\n", "观察熵随分布均匀程度变化的规律。"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n\ndef entropy(p):\n    p = np.asarray(p)\n    p = p[p > 0]\n    return -np.sum(p * np.log2(p))\n\nps = [np.array([0.5, 0.5]), np.array([0.9, 0.1]), np.array([0.7, 0.2, 0.1])]\nfor p in ps:\n    print(f'p={p}, 熵={entropy(p):.3f} bits')\n# TODO: 尝试更多分布，并绘制熵随 p 变化的曲线\n"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 2. 交叉熵与 KL 散度\n", "实现二分类交叉熵与 KL 计算，体会非负性。"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def binary_cross_entropy(y_true, y_pred, eps=1e-8):\n    y_pred = np.clip(y_pred, eps, 1 - eps)\n    return -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n\n# TODO: 增加 KL 散度计算，尝试不同预测分布\nprint(binary_cross_entropy(np.array([1, 0]), np.array([0.8, 0.2])))\n"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Softmax 多分类实验\n", "在二维可视化数据上训练 softmax 回归，比较 MSE 与交叉熵。"]},
  {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# TODO: 生成三类可分数据，手写 softmax 回归训练，绘制决策边界与损失变化\n"]}
 ],
 "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}},
 "nbformat": 4,
 "nbformat_minor": 5
}
